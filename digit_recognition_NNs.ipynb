{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digit Recognition using Neural Networks\n",
    "We wish to train Neural networks to recognise handwritten digits. We use the MNIST database of handwritten digits which consists of a training set of 60,000 examples, and a test set of 10,000 examples. First, we import the data and modify it to a suitable format to train the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "image_size = 28\n",
    "no_of_labels = 10   # digits 0-9 inc.\n",
    "image_pixels = image_size*image_size # images are 28x28 pixels\n",
    "\n",
    "test_images = np.loadtxt('.\mnist_train.csv')\n",
    "train_images = np.loadtxt('.\mnist_test.csv)\n",
    "\n",
    "# test_images and train_images are arrays where each row contains 785 numbers\n",
    "# first number in each row is the label (i.e digit from 0-9 depicted in image). Remaining 784 numbers are the pixels of the 28x28 image\n",
    "\n",
    "# We wish to have the labels in a one-hot representation  \n",
    "# extract labels from data and flatten into 1D array\n",
    "train_labels = np.asfarray(train_images[:, 0])\n",
    "test_labels = np.asfarray(test_images[:, 0])\n",
    "\n",
    "identity_matrix = np.eye(no_of_labels)  # 10x10 identity matrix (ith row is one-hot encoded vector for digit i-1)\n",
    "\n",
    "train_labels_one_hot = identity_matrix[train_labels.astype(int)]\n",
    "test_labels_one_hot = identity_matrix[test_labels.astype(int)]\n",
    "\n",
    "# pixels take values in range 0 - 255 inc. We map these values into an interval [0.01, 1]\n",
    "# This means we avoid 0s as input values which can prevent weight updates\n",
    "f = (1 - 0.01)/255\n",
    "c = 0.01\n",
    "test_pixels = np.asfarray(test_images[:, 1:])*f + c\n",
    "train_pixels = np.asfarray(train_images[:, 1:])*f + c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artificial Neural Network from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 completed in 11.10 seconds\n",
      "Epoch 2/30 completed in 19.17 seconds\n",
      "Epoch 3/30 completed in 18.09 seconds\n",
      "Epoch 4/30 completed in 19.22 seconds\n",
      "Epoch 5/30 completed in 18.34 seconds\n",
      "Epoch 6/30 completed in 18.02 seconds\n",
      "Epoch 7/30 completed in 18.95 seconds\n",
      "Epoch 8/30 completed in 19.56 seconds\n",
      "Epoch 9/30 completed in 19.44 seconds\n",
      "Epoch 10/30 completed in 17.53 seconds\n",
      "Epoch 11/30 completed in 19.21 seconds\n",
      "Epoch 12/30 completed in 18.63 seconds\n",
      "Epoch 13/30 completed in 19.00 seconds\n",
      "Epoch 14/30 completed in 15.23 seconds\n",
      "Epoch 15/30 completed in 25.17 seconds\n",
      "Epoch 16/30 completed in 25.21 seconds\n",
      "Epoch 17/30 completed in 25.89 seconds\n",
      "Epoch 18/30 completed in 27.04 seconds\n",
      "Epoch 19/30 completed in 25.42 seconds\n",
      "Epoch 20/30 completed in 25.87 seconds\n",
      "Epoch 21/30 completed in 26.82 seconds\n",
      "Epoch 22/30 completed in 32.10 seconds\n",
      "Epoch 23/30 completed in 26.72 seconds\n",
      "Epoch 24/30 completed in 27.31 seconds\n",
      "Epoch 25/30 completed in 26.64 seconds\n",
      "Epoch 26/30 completed in 25.35 seconds\n",
      "Epoch 27/30 completed in 26.27 seconds\n",
      "Epoch 28/30 completed in 23.78 seconds\n",
      "Epoch 29/30 completed in 26.10 seconds\n",
      "Epoch 30/30 completed in 25.16 seconds\n",
      "Training completed in 672.31 seconds\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Accuracy of ANN from scratch on test data: 95.96%\n"
     ]
    }
   ],
   "source": [
    "import ArtificialNeuralNetwork\n",
    "\n",
    "# We want data to be a list of tuples (x, y) \n",
    "# where x are (784,1) Numpy.ndarrays (input vectors) and y are the corresponding labels in one-hot representation\n",
    "\n",
    "training_data = []\n",
    "for input_vector, label in zip(train_pixels, train_labels_one_hot):\n",
    "    reshaped_input = np.reshape(input_vector, (784,1)) \n",
    "    reshaped_label = np.reshape(label, (10,1))  \n",
    "    training_data.append((reshaped_input,reshaped_label))\n",
    "\n",
    "test_data = []\n",
    "for input_vector, label in zip(test_pixels, test_labels_one_hot):\n",
    "    reshaped_input = np.reshape(input_vector, (784,1))\n",
    "    reshaped_label = np.reshape(label, (10,1))\n",
    "    test_data.append((reshaped_input,reshaped_label))\n",
    "\n",
    "\n",
    "ann = ArtificialNeuralNetwork.NeuralNetwork([784, 30, 10])  # 784 neurons in input layer since input vectors are (784,1) numpy.ndarrays                 \n",
    "                                                                # 10 neurons in output layer since labels are (10,1) numpy.ndarray\n",
    "\n",
    "# We choose values of hyperparameters\n",
    "epochs = 30\n",
    "mini_batch_size = 32\n",
    "learning_rate = 0.1\n",
    "regularisation_parameter = 5.0\n",
    "\n",
    "ann.train(training_data, epochs, learning_rate, mini_batch_size, regularisation_parameter)\n",
    "\n",
    "# Now lets test the performance on our test data\n",
    "ann_accuracy = ann.evaluate(test_data)\n",
    "print('-'*100)\n",
    "print(f'Accuracy of ANN from scratch on test data: {ann_accuracy*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 completed in 1283.41 seconds: Loss = 0.0745, accuracy = 87.89%\n",
      "Epoch 2/10 completed in 538.06 seconds: Loss = 0.0342, accuracy = 92.89%\n",
      "Epoch 3/10 completed in 325.97 seconds: Loss = 0.0248, accuracy = 94.18%\n",
      "Epoch 4/10 completed in 324.95 seconds: Loss = 0.0206, accuracy = 94.77%\n",
      "Epoch 5/10 completed in 330.10 seconds: Loss = 0.0178, accuracy = 95.39%\n",
      "Epoch 6/10 completed in 327.23 seconds: Loss = 0.0165, accuracy = 95.62%\n",
      "Epoch 7/10 completed in 328.60 seconds: Loss = 0.0152, accuracy = 95.89%\n",
      "Epoch 8/10 completed in 381.74 seconds: Loss = 0.0145, accuracy = 96.05%\n",
      "Epoch 9/10 completed in 344.78 seconds: Loss = 0.0134, accuracy = 96.22%\n",
      "Epoch 10/10 completed in 335.57 seconds: Loss = 0.0132, accuracy = 96.41%\n",
      "Training completed in 4520.41 seconds\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Accuracy of CNN from scratch on test data: 95.54%\n"
     ]
    }
   ],
   "source": [
    "import ConvolutionalNeuralNetwork\n",
    "\n",
    "# We want data to be a list of tuples (x, y) \n",
    "# where x are (1,28,28) Numpy.ndarrays (input tensors) and y are the corresponding labels in one-hot representation\n",
    "\n",
    "training_data = []\n",
    "for input_vector, label in zip(train_pixels, train_labels_one_hot):\n",
    "    reshaped_input = np.reshape(input_vector, (1,28,28)) \n",
    "    reshaped_label = np.reshape(label, (10,1))  \n",
    "    training_data.append((reshaped_input,reshaped_label))\n",
    "\n",
    "test_data = []\n",
    "for input_vector, label in zip(test_pixels, test_labels_one_hot):\n",
    "    reshaped_input = np.reshape(input_vector, (1, 28, 28))\n",
    "    reshaped_label = np.reshape(label, (10,1))\n",
    "    test_data.append((reshaped_input,reshaped_label))\n",
    "\n",
    "conv = ConvolutionalNeuralNetwork.Convolution((1, 28, 28), 3, 5) # 5 kernels (which are 3x3 matrices), input shape (1, 28, 28)\n",
    "                                                                 # Ouput size will be (5, 26, 26)\n",
    "\n",
    "pool = ConvolutionalNeuralNetwork.MaxPool(2) # pool patches are size (2,2)\n",
    "                                             # Output size will be (5, 13, 13)\n",
    "\n",
    "full = ConvolutionalNeuralNetwork.FullyConnected(845,10) # flatten (5, 13, 13) to (845, 1)\n",
    "                                                         # Require output to be shape (10, 1)\n",
    "                                                         # 845 input neurons, 10 output neurons\n",
    "\n",
    "# Choose value of hyperparameters\n",
    "\n",
    "epochs = 10\n",
    "mini_batch_size = 48\n",
    "learning_rate = 0.1\n",
    "\n",
    "ConvolutionalNeuralNetwork.train_network(training_data, conv, pool, full, learning_rate, epochs, mini_batch_size)\n",
    "\n",
    "# test performance on test data\n",
    "cnn_accuracy = ConvolutionalNeuralNetwork.evaluate(test_data, conv, pool, full)\n",
    "print('-'*100)\n",
    "print(f'Accuracy of CNN from scratch on test data: {cnn_accuracy*100:.2f}%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,), (0.5,))])  # Normalize grayscale images between -1 and 1\n",
    "\n",
    "# Download and load the training and test data\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create data loaders for batching\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "\n",
    "class ANN(nn.Module):\n",
    "    \"An Artificial Neural Network. Inherits from the torch.nn.module\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"Initialise the layers of the NN\"\n",
    "        super(ANN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 128)    # Fully connected layer (28x28 input size, 128 neurons)\n",
    "        self.fc2 = nn.Linear(128, 64)   # Fully connected layer (128 input size, 64 neurons)\n",
    "        self.fc3 = nn.Linear(64, 10)    # Fully connected layer (64 input size, 10 output classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"The feed forward part of the network\"\n",
    "        x = x.view(x.shape[0], -1)  # Flattens input data (first dimension reamins batch size)\n",
    "        x = F.relu(self.fc1(x)) # First layer with ReLU activation function\n",
    "        x = F.relu(self.fc2(x)) # Second layer with ReLU activation function\n",
    "        x = self.fc3(x) # Output layer\n",
    "        return x\n",
    "    \n",
    "class CNN(nn.Module):\n",
    "    \"A Convolutional Neural Network. Inherits from the torch.nn.module\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"Initialise the layers of the NN\"\n",
    "        super(CNN, self).__init__()\n",
    "        # padding adds one pixel around edge of image so it retains original dimensions after convolution layer\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, padding=1) # input: (1, 28, 28), output: (32, 28, 28)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2,stride=2) # input: (32, 28, 28), output: (32, 14, 14)\n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1) # input: (32, 14, 14), output: (64, 14, 14) \n",
    "        #self.pool - input (64, 14, 14), output: (64, 7, 7)\n",
    "        self.fc1 = nn.Linear(64*7*7, 128) # input (64,7,7), output:(128, 1)\n",
    "        self.fc2 = nn.Linear(128, 10)   # input: (128, 1), output: (10,1)\n",
    "\n",
    "    def forward(self,x):\n",
    "        \"The feed forward part of the network\"\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64*7*7) # flatten data\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x) # no need for softmax as this is implemented automatically by CrossEntropyLoss\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANN using PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25 completed in 8.588006734848022 seconds: Loss = 0.4060\n",
      "Epoch 2/25 completed in 9.070082426071167 seconds: Loss = 0.1906\n",
      "Epoch 3/25 completed in 8.331225872039795 seconds: Loss = 0.1388\n",
      "Epoch 4/25 completed in 8.129310131072998 seconds: Loss = 0.1143\n",
      "Epoch 5/25 completed in 8.079305648803711 seconds: Loss = 0.0952\n",
      "Epoch 6/25 completed in 8.269386053085327 seconds: Loss = 0.0814\n",
      "Epoch 7/25 completed in 8.201983213424683 seconds: Loss = 0.0723\n",
      "Epoch 8/25 completed in 8.393942832946777 seconds: Loss = 0.0653\n",
      "Epoch 9/25 completed in 8.366328716278076 seconds: Loss = 0.0579\n",
      "Epoch 10/25 completed in 8.231220245361328 seconds: Loss = 0.0526\n",
      "Epoch 11/25 completed in 8.567504405975342 seconds: Loss = 0.0494\n",
      "Epoch 12/25 completed in 8.270044326782227 seconds: Loss = 0.0461\n",
      "Epoch 13/25 completed in 8.419809103012085 seconds: Loss = 0.0414\n",
      "Epoch 14/25 completed in 8.360164403915405 seconds: Loss = 0.0415\n",
      "Epoch 15/25 completed in 8.183289766311646 seconds: Loss = 0.0353\n",
      "Epoch 16/25 completed in 8.58333444595337 seconds: Loss = 0.0350\n",
      "Epoch 17/25 completed in 9.402926921844482 seconds: Loss = 0.0316\n",
      "Epoch 18/25 completed in 8.983858108520508 seconds: Loss = 0.0319\n",
      "Epoch 19/25 completed in 8.91662883758545 seconds: Loss = 0.0293\n",
      "Epoch 20/25 completed in 8.99228811264038 seconds: Loss = 0.0279\n",
      "Epoch 21/25 completed in 9.047439813613892 seconds: Loss = 0.0284\n",
      "Epoch 22/25 completed in 8.903388977050781 seconds: Loss = 0.0250\n",
      "Epoch 23/25 completed in 9.07669448852539 seconds: Loss = 0.0253\n",
      "Epoch 24/25 completed in 9.26015567779541 seconds: Loss = 0.0224\n",
      "Epoch 25/25 completed in 8.962663412094116 seconds: Loss = 0.0233\n",
      "Finished Training\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Accuracy of PyTorch ANN on test data: 97.45%\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "ann = ANN()\n",
    "\n",
    "ann_criterion = nn.CrossEntropyLoss()   # Cross entropy loss function with softmax\n",
    "ann_optimiser = optim.Adam(ann.parameters(), lr=0.001)   # Adam optimiser (outperforms SGD optimiser)\n",
    "\n",
    "# training loop\n",
    "ann_epochs = 25\n",
    "for epoch in range(ann_epochs): # loop over the dataset multiple times\n",
    "    time1 = time.time()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        ann_optimiser.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = ann(inputs)\n",
    "        loss = ann_criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        ann_optimiser.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    time2 = time.time()\n",
    "    t = time2 - time1\n",
    "    print(f'Epoch {epoch + 1}/{ann_epochs} completed in {t} seconds: Loss = {running_loss/len(train_loader):.4f}')\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        outputs = ann(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('-'*100)\n",
    "print(f'Accuracy of PyTorch ANN on test data: {correct/total*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CNN using PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 completed in 34.74462175369263 seconds: Loss = 0.1634\n",
      "Epoch 2/15 completed in 36.201146602630615 seconds: Loss = 0.0483\n",
      "Epoch 3/15 completed in 34.02916407585144 seconds: Loss = 0.0330\n",
      "Epoch 4/15 completed in 34.21970081329346 seconds: Loss = 0.0259\n",
      "Epoch 5/15 completed in 35.25494742393494 seconds: Loss = 0.0190\n",
      "Epoch 6/15 completed in 34.875287771224976 seconds: Loss = 0.0158\n",
      "Epoch 7/15 completed in 34.4595422744751 seconds: Loss = 0.0118\n",
      "Epoch 8/15 completed in 34.66200566291809 seconds: Loss = 0.0103\n",
      "Epoch 9/15 completed in 34.44820165634155 seconds: Loss = 0.0089\n",
      "Epoch 10/15 completed in 34.75017166137695 seconds: Loss = 0.0072\n",
      "Epoch 11/15 completed in 34.59986686706543 seconds: Loss = 0.0073\n",
      "Epoch 12/15 completed in 34.320034980773926 seconds: Loss = 0.0056\n",
      "Epoch 13/15 completed in 34.533716440200806 seconds: Loss = 0.0062\n",
      "Epoch 14/15 completed in 34.60637831687927 seconds: Loss = 0.0053\n",
      "Epoch 15/15 completed in 34.62997603416443 seconds: Loss = 0.0038\n",
      "Finished Training\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Accuracy of PyTorch CNN on test data: 98.92%\n"
     ]
    }
   ],
   "source": [
    "cnn = CNN()\n",
    "\n",
    "cnn_criterion = nn.CrossEntropyLoss()   # Cross entropy loss function with softmax\n",
    "cnn_optimiser = optim.Adam(cnn.parameters(), lr=0.001)   # Adam optimiser (outperforms SGD optimiser)\n",
    "\n",
    "# training loop\n",
    "cnn_epochs = 15\n",
    "for epoch in range(cnn_epochs): # loop over the dataset multiple times\n",
    "    time1 = time.time()\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data\n",
    "        cnn_optimiser.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = cnn(inputs)\n",
    "        loss = cnn_criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        cnn_optimiser.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    time2 = time.time()\n",
    "    t = time2 - time1\n",
    "    print(f'Epoch {epoch + 1}/{cnn_epochs} completed in {t} seconds: Loss = {running_loss/len(train_loader):.4f}')\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:\n",
    "        inputs, labels = data\n",
    "        outputs = cnn(inputs)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('-'*100)\n",
    "print(f'Accuracy of PyTorch CNN on test data: {correct/total*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = x_train / 255.0 # shape (60000, 28, 28, 1)\n",
    "x_test = x_test / 255.0 # shape (10000, 28, 28, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can use Keras' sequential API to build a model. Very quick for simple models like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8530 - loss: 0.5081\n",
      "Epoch 2/20\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9630 - loss: 0.1260\n",
      "Epoch 3/20\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9741 - loss: 0.0835\n",
      "Epoch 4/20\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9825 - loss: 0.0575\n",
      "Epoch 5/20\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9865 - loss: 0.0438\n",
      "Epoch 6/20\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9894 - loss: 0.0337\n",
      "Epoch 7/20\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.9912 - loss: 0.0291\n",
      "Epoch 8/20\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 8ms/step - accuracy: 0.9933 - loss: 0.0206\n",
      "Epoch 9/20\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9943 - loss: 0.0184\n",
      "Epoch 10/20\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9955 - loss: 0.0150\n",
      "Epoch 11/20\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9943 - loss: 0.0171\n",
      "Epoch 12/20\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9955 - loss: 0.0133\n",
      "Epoch 13/20\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9956 - loss: 0.0133\n",
      "Epoch 14/20\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9965 - loss: 0.0103\n",
      "Epoch 15/20\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9973 - loss: 0.0077\n",
      "Epoch 16/20\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.9958 - loss: 0.0117\n",
      "Epoch 17/20\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9978 - loss: 0.0071\n",
      "Epoch 18/20\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.9980 - loss: 0.0065\n",
      "Epoch 19/20\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9962 - loss: 0.0106\n",
      "Epoch 20/20\n",
      "\u001b[1m938/938\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9986 - loss: 0.0045\n",
      "313/313 - 1s - 4ms/step - accuracy: 0.9775 - loss: 0.1152\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Accuracy of TensorFlow ANN on test data: 97.75%\n"
     ]
    }
   ],
   "source": [
    "ann_model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Input(shape=(28,28,1)),\n",
    "  tf.keras.layers.Flatten(),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(64, activation='relu'),\n",
    "  tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "ann_model.compile(optimizer='adam', loss=loss_fn, metrics=['accuracy'])\n",
    "\n",
    "ann_model.fit(x_train, y_train, epochs=20, batch_size=64)   # train model\n",
    "test_loss, test_acc = ann_model.evaluate(x_test,  y_test, verbose=2)\n",
    "\n",
    "print('-'*100)\n",
    "print(f'Accuracy of TensorFlow ANN on test data: {test_acc*100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can also subclass TensorFlow’s Model class to create a custom model. This is a bit more involved but allows more flexibility when making more complex models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 completed in 15.630149602890015 seconds: Loss: 0.1266, Accuracy: 96.14%\n",
      "Epoch 2/15 completed in 11.930044651031494 seconds: Loss: 0.0426, Accuracy: 98.70%\n",
      "Epoch 3/15 completed in 15.186951398849487 seconds: Loss: 0.0285, Accuracy: 99.12%\n",
      "Epoch 4/15 completed in 31.744410037994385 seconds: Loss: 0.0210, Accuracy: 99.32%\n",
      "Epoch 5/15 completed in 35.96498942375183 seconds: Loss: 0.0157, Accuracy: 99.48%\n",
      "Epoch 6/15 completed in 35.397237062454224 seconds: Loss: 0.0125, Accuracy: 99.58%\n",
      "Epoch 7/15 completed in 31.76819157600403 seconds: Loss: 0.0094, Accuracy: 99.68%\n",
      "Epoch 8/15 completed in 23.586084842681885 seconds: Loss: 0.0086, Accuracy: 99.72%\n",
      "Epoch 9/15 completed in 39.886969566345215 seconds: Loss: 0.0080, Accuracy: 99.72%\n",
      "Epoch 10/15 completed in 40.311445236206055 seconds: Loss: 0.0058, Accuracy: 99.80%\n",
      "Epoch 11/15 completed in 39.89866018295288 seconds: Loss: 0.0061, Accuracy: 99.80%\n",
      "Epoch 12/15 completed in 35.35233473777771 seconds: Loss: 0.0051, Accuracy: 99.84%\n",
      "Epoch 13/15 completed in 11.71713924407959 seconds: Loss: 0.0047, Accuracy: 99.86%\n",
      "Epoch 14/15 completed in 12.265791654586792 seconds: Loss: 0.0038, Accuracy: 99.89%\n",
      "Epoch 15/15 completed in 12.513668537139893 seconds: Loss: 0.0043, Accuracy: 99.87%\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Accuracy of TensorFlow CNN on test data: 99.13%\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "# add channels dimension\n",
    "x_train = x_train[..., tf.newaxis].astype(\"float32\")\n",
    "x_test = x_test[..., tf.newaxis].astype(\"float32\")\n",
    "\n",
    "class CNN(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv2D(filters=32, kernel_size=3, activation='relu')\n",
    "        self.pool = MaxPooling2D((2,2))\n",
    "        self.conv2 = Conv2D(filters=64, kernel_size=3, activation='relu')\n",
    "        self.flatten = Flatten()\n",
    "        self.dense1 = Dense(128,activation='relu')\n",
    "        self.dense2 = Dense(10)\n",
    "\n",
    "    def call(self,x):\n",
    "        x = self.pool(self.conv1(x))\n",
    "        x = self.pool(self.conv2(x))\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)  # No activation needed as we use from_logits=True in the loss function\n",
    "        return x\n",
    "    \n",
    "cnn_model = CNN()\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimiser = tf.keras.optimizers.Adam()\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(10000).batch(32)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)\n",
    "\n",
    "# use tf.GradientTape to train model\n",
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "  with tf.GradientTape() as tape:\n",
    "    \n",
    "    predictions = cnn_model(images, training=True)\n",
    "    loss = loss_object(labels, predictions)\n",
    "  gradients = tape.gradient(loss, cnn_model.trainable_variables)\n",
    "  optimiser.apply_gradients(zip(gradients, cnn_model.trainable_variables))\n",
    "\n",
    "  train_loss(loss)\n",
    "  train_accuracy(labels, predictions)\n",
    "\n",
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "  \n",
    "  predictions = cnn_model(images, training=False)\n",
    "  t_loss = loss_object(labels, predictions)\n",
    "\n",
    "  test_loss(t_loss)\n",
    "  test_accuracy(labels, predictions)\n",
    "\n",
    "epochs = 15\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  time1 = time.time()\n",
    "  # Reset the metrics at the start of each epoch\n",
    "  train_loss.reset_state()\n",
    "  train_accuracy.reset_state()\n",
    "\n",
    "  # Training loop\n",
    "  for images, labels in train_ds:  \n",
    "    train_step(images, labels)\n",
    "\n",
    "  time2 = time.time()\n",
    "  t = time2 - time1\n",
    "  print(f'Epoch {epoch + 1}/{epochs} completed in {t} seconds: Loss: {train_loss.result():.4f}, Accuracy: {train_accuracy.result() * 100:.2f}%')\n",
    "\n",
    "# Perform testing after all training epochs\n",
    "for test_images, test_labels in test_ds: \n",
    "    test_step(test_images, test_labels)\n",
    "\n",
    "print('-'*100)\n",
    "print(f'Accuracy of TensorFlow CNN on test data: {test_accuracy.result() * 100:.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
